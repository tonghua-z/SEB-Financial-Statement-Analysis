{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from transformers import RobertaModel, RobertaConfig, RobertaTokenizer\n",
    "from transformers import AlbertModel, AlbertConfig, AlbertTokenizer\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from torch.utils.data import Dataset,DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_UkeC7SG2krJ",
    "outputId": "e20220fa-7ba5-4f59-d84d-772e95f18eff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>targetPriceChange</th>\n",
       "      <th>bullets</th>\n",
       "      <th>recommendationId</th>\n",
       "      <th>total_text</th>\n",
       "      <th>prepped_total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18007</th>\n",
       "      <td>2014-02-04 08:23:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'Kesko reported Q4 clean EBIT of EUR...</td>\n",
       "      <td>2</td>\n",
       "      <td>Q4 EBIT a bit better than we and consensus exp...</td>\n",
       "      <td>fourth quarter earnings before interest and ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>2012-02-08 06:50:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'We expect Storebrand to report a ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>Target price cut due to proposal to limit the ...</td>\n",
       "      <td>target price cut due to proposal to limit the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>2012-07-09 15:39:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'We expect Lassila &amp; Tikanoja to rep...</td>\n",
       "      <td>1</td>\n",
       "      <td>We estimate a Q2 EBIT of EUR 9.9m. We expect L...</td>\n",
       "      <td>we estimate a second quarter earnings before i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>2011-05-11 07:05:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'Hexagon’s underlying EBIT was 5% ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>Underlying EBIT 5% better than expected; upgra...</td>\n",
       "      <td>underlying earnings before interest and taxes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18370</th>\n",
       "      <td>2014-07-09 14:52:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'We have included the acquisition of...</td>\n",
       "      <td>2</td>\n",
       "      <td>Higher target price due to acquisition and low...</td>\n",
       "      <td>higher target price due to acquisition and low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>2011-07-29 06:40:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'We have cut our EPS forecast by 7% ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Estimates, target slightly down – opinion unch...</td>\n",
       "      <td>estimates  target slightly down – opinion unch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221</th>\n",
       "      <td>2017-04-28 06:24:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'The Q1 2017 results added little va...</td>\n",
       "      <td>2</td>\n",
       "      <td>Quarterly results are non-events – Q1 2017 was...</td>\n",
       "      <td>quarterly results are non events – first quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>2011-04-12 07:07:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'After a 23% y-o-y increase in passe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly better Q2 than we expected. After a 2...</td>\n",
       "      <td>slightly better second quarter than we expecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13260</th>\n",
       "      <td>2019-09-05 05:51:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': '', 'header': 'CEO Lotta Lyrå has co...</td>\n",
       "      <td>1</td>\n",
       "      <td>CEO Lotta Lyrå has confirmed a step-up in reta...</td>\n",
       "      <td>chief executive officer lotta lyrå has confirm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11436</th>\n",
       "      <td>2018-11-05 13:08:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': '', 'header': 'Eli Lilly has just th...</td>\n",
       "      <td>1</td>\n",
       "      <td>Eli Lilly has just that Trulicity significantl...</td>\n",
       "      <td>eli &lt;company&gt; has just that trulicity signific...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  publishedDate  targetPriceChange  \\\n",
       "18007 2014-02-04 08:23:00+00:00              False   \n",
       "3798  2012-02-08 06:50:00+00:00              False   \n",
       "4484  2012-07-09 15:39:00+00:00              False   \n",
       "2518  2011-05-11 07:05:00+00:00              False   \n",
       "18370 2014-07-09 14:52:00+00:00              False   \n",
       "2884  2011-07-29 06:40:00+00:00              False   \n",
       "20221 2017-04-28 06:24:00+00:00              False   \n",
       "2214  2011-04-12 07:07:00+00:00              False   \n",
       "13260 2019-09-05 05:51:00+00:00              False   \n",
       "11436 2018-11-05 13:08:00+00:00              False   \n",
       "\n",
       "                                                 bullets  recommendationId  \\\n",
       "18007  [{'text': 'Kesko reported Q4 clean EBIT of EUR...                 2   \n",
       "3798   [{'text': 'We expect Storebrand to report a ne...                 1   \n",
       "4484   [{'text': 'We expect Lassila & Tikanoja to rep...                 1   \n",
       "2518   [{'text': 'Hexagon’s underlying EBIT was 5% ab...                 1   \n",
       "18370  [{'text': 'We have included the acquisition of...                 2   \n",
       "2884   [{'text': 'We have cut our EPS forecast by 7% ...                 1   \n",
       "20221  [{'text': 'The Q1 2017 results added little va...                 2   \n",
       "2214   [{'text': 'After a 23% y-o-y increase in passe...                 1   \n",
       "13260  [{'text': '', 'header': 'CEO Lotta Lyrå has co...                 1   \n",
       "11436  [{'text': '', 'header': 'Eli Lilly has just th...                 1   \n",
       "\n",
       "                                              total_text  \\\n",
       "18007  Q4 EBIT a bit better than we and consensus exp...   \n",
       "3798   Target price cut due to proposal to limit the ...   \n",
       "4484   We estimate a Q2 EBIT of EUR 9.9m. We expect L...   \n",
       "2518   Underlying EBIT 5% better than expected; upgra...   \n",
       "18370  Higher target price due to acquisition and low...   \n",
       "2884   Estimates, target slightly down – opinion unch...   \n",
       "20221  Quarterly results are non-events – Q1 2017 was...   \n",
       "2214   Slightly better Q2 than we expected. After a 2...   \n",
       "13260  CEO Lotta Lyrå has confirmed a step-up in reta...   \n",
       "11436  Eli Lilly has just that Trulicity significantl...   \n",
       "\n",
       "                                      prepped_total_text  \n",
       "18007  fourth quarter earnings before interest and ta...  \n",
       "3798   target price cut due to proposal to limit the ...  \n",
       "4484   we estimate a second quarter earnings before i...  \n",
       "2518   underlying earnings before interest and taxes ...  \n",
       "18370  higher target price due to acquisition and low...  \n",
       "2884   estimates  target slightly down – opinion unch...  \n",
       "20221  quarterly results are non events – first quart...  \n",
       "2214   slightly better second quarter than we expecte...  \n",
       "13260  chief executive officer lotta lyrå has confirm...  \n",
       "11436  eli <company> has just that trulicity signific...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_reports = pd.read_pickle(\"reports_processed.pkl\")\n",
    "df_reports.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with 0 label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>targetPriceChange</th>\n",
       "      <th>bullets</th>\n",
       "      <th>recommendationId</th>\n",
       "      <th>total_text</th>\n",
       "      <th>prepped_total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2010-03-01 06:23:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'Golar LNG reported a Q4 2009 EBITDA...</td>\n",
       "      <td>0</td>\n",
       "      <td>Q4: Early Winter lifts EBIT. Golar LNG reporte...</td>\n",
       "      <td>fourth quarter  early winter lifts earnings be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2010-03-01 06:23:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'CTC Media, in which MTG owns 39%, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>Significantly outpaced the Russian TV ad marke...</td>\n",
       "      <td>significantly outpaced the russian tv ad marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2010-03-01 09:25:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'We expect BoConcept to slightly upg...</td>\n",
       "      <td>0</td>\n",
       "      <td>We believe an earnings upgrade is on the cards...</td>\n",
       "      <td>we believe an earnings upgrade is on the cards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2010-03-01 10:26:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'Questerre reported very good horizo...</td>\n",
       "      <td>0</td>\n",
       "      <td>First middle-Utica horizontal well exceeds all...</td>\n",
       "      <td>first middle utica horizontal well exceeds &lt;cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2010-03-01 11:01:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'text': 'Thrane &amp; Thrane reported sales of D...</td>\n",
       "      <td>0</td>\n",
       "      <td>Guidance narrowed down as expected. Thrane &amp; T...</td>\n",
       "      <td>guidance narrowed down as expected  &lt;company&gt; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                publishedDate  targetPriceChange  \\\n",
       "376 2010-03-01 06:23:00+00:00              False   \n",
       "377 2010-03-01 06:23:00+00:00              False   \n",
       "378 2010-03-01 09:25:00+00:00              False   \n",
       "379 2010-03-01 10:26:00+00:00              False   \n",
       "380 2010-03-01 11:01:00+00:00              False   \n",
       "\n",
       "                                               bullets  recommendationId  \\\n",
       "376  [{'text': 'Golar LNG reported a Q4 2009 EBITDA...                 0   \n",
       "377  [{'text': 'CTC Media, in which MTG owns 39%, r...                 0   \n",
       "378  [{'text': 'We expect BoConcept to slightly upg...                 0   \n",
       "379  [{'text': 'Questerre reported very good horizo...                 0   \n",
       "380  [{'text': 'Thrane & Thrane reported sales of D...                 0   \n",
       "\n",
       "                                            total_text  \\\n",
       "376  Q4: Early Winter lifts EBIT. Golar LNG reporte...   \n",
       "377  Significantly outpaced the Russian TV ad marke...   \n",
       "378  We believe an earnings upgrade is on the cards...   \n",
       "379  First middle-Utica horizontal well exceeds all...   \n",
       "380  Guidance narrowed down as expected. Thrane & T...   \n",
       "\n",
       "                                    prepped_total_text  \n",
       "376  fourth quarter  early winter lifts earnings be...  \n",
       "377  significantly outpaced the russian tv ad marke...  \n",
       "378  we believe an earnings upgrade is on the cards...  \n",
       "379  first middle utica horizontal well exceeds <cu...  \n",
       "380  guidance narrowed down as expected  <company> ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reports = df_reports[df_reports[\"recommendationId\"] >= 1]\n",
    "df_reports[\"recommendationId\"] = df_reports[\"recommendationId\"] - 1\n",
    "df_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with null text or too short text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empty = df_reports[df_reports['prepped_total_text'].map(len) < 100]\n",
    "df_reports =df_reports.drop(df_reports[df_reports[\"prepped_total_text\"].map(len) < 100].index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df_reports.prepped_total_text.values\n",
    "labels = df_reports.recommendationId.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z474sSC6oe7A",
    "outputId": "5cac4edc-a347-4d50-99d6-9dd1f4fd99d2"
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('/home/jupyter/pretrained-models/roberta-large/')\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/jupyter/pretrained-models/fin-bert/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example outputs from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dLIbudgfh6F0",
    "outputId": "73326d52-d489-4e06-b706-99e6a5f7d57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  fourth quarter  early winter lifts earnings before interest and taxes  <company> reported a fourth quarter <year> earnings before interest  taxes  depreciation  and amortization of <currency> <number> million  against our <currency> <number> million forecast and the <currency> <number> million consensus  inquiry   the pre tax deviations were similar  the better than expected results were caused by higher earnings for the spot trading of vessels in its daughter company  <company> energy  management indicates that utilisation in the spot market will be low for the first half of <year> and as a consequence we have cut our <year> earnings per share forecast by <percent>  however  dividend payments will start earlier  in second quarter <year> instead of third quarter <year>  risks and directions  <company> charters largely take care of its cash flow  which we believe are attractively priced in the market  we also consider the changes in the daughter company <company> energy  which we do not cover  to be good for golar shareholders  the company has backed away from complex floating lng production and extensive investments in liquefaction  instead  the company has retained earnings focused on fsrus  which have significantly shorter lead times  more specific tenders and lower technical risk  the fourth quarter <year> presentation included a chart showing lng overcapacity for some years  which in turn should be encouraging for lng importers and fsru operators  updated sotp and attractive dividends coming  as <company> energy is an adequately traded stock  there is no reason to penalise <company> for its holding  we estimate <company> will generate <currency> <number_range>  million of cash flow  just above <currency> <number> per share annually of free cash  after debt amortisation  our updated sotp returns <currency> <number> per share  down from <number>  the changes relate to the price of gole  we reiterate our buy rating  with a revised target price of <currency> <number>  \n",
      "Tokenized:  ['fourth', 'quarter', 'early', 'winter', 'lifts', 'earnings', 'before', 'interest', 'and', 'taxes', '<', 'company', '>', 'reported', 'a', 'fourth', 'quarter', '<', 'year', '>', 'earnings', 'before', 'interest', 'taxes', 'de', '##pre', '##ciation', 'and', 'amor', '##ti', '##zation', 'of', '<', 'currency', '>', '<', 'number', '>', 'million', 'against', 'our', '<', 'currency', '>', '<', 'number', '>', 'million', 'forecast', 'and', 'the', '<', 'currency', '>', '<', 'number', '>', 'million', 'consensus', 'inquiry', 'the', 'pre', 'tax', 'deviation', '##s', 'were', 'similar', 'the', 'better', 'than', 'expected', 'results', 'were', 'caused', 'by', 'higher', 'earnings', 'for', 'the', 'spot', 'trading', 'of', 'vessels', 'in', 'its', 'daughter', 'company', '<', 'company', '>', 'energy', 'management', 'indicates', 'that', 'ut', '##ilis', '##ation', 'in', 'the', 'spot', 'market', 'will', 'be', 'low', 'for', 'the', 'first', 'half', 'of', '<', 'year', '>', 'and', 'as', 'a', 'consequence', 'we', 'have', 'cut', 'our', '<', 'year', '>', 'earnings', 'per', 'share', 'forecast', 'by', '<', 'percent', '>', 'however', 'divide', '##nd', 'payments', 'will', 'start', 'earlier', 'in', 'second', 'quarter', '<', 'year', '>', 'instead', 'of', 'third', 'quarter', '<', 'year', '>', 'risks', 'and', 'directions', '<', 'company', '>', 'charters', 'largely', 'take', 'care', 'of', 'its', 'cash', 'flow', 'which', 'we', 'believe', 'are', 'attractive', '##ly', 'priced', 'in', 'the', 'market', 'we', 'also', 'consider', 'the', 'changes', 'in', 'the', 'daughter', 'company', '<', 'company', '>', 'energy', 'which', 'we', 'do', 'not', 'cover', 'to', 'be', 'good', 'for', 'go', '##lar', 'shareholders', 'the', 'company', 'has', 'backed', 'away', 'from', 'complex', 'floating', 'l', '##ng', 'production', 'and', 'extensive', 'investments', 'in', 'li', '##que', '##fa', '##ction', 'instead', 'the', 'company', 'has', 'retained', 'earnings', 'focused', 'on', 'f', '##sr', '##us', 'which', 'have', 'significantly', 'shorter', 'lead', 'times', 'more', 'specific', 'tender', '##s', 'and', 'lower', 'technical', 'risk', 'the', 'fourth', 'quarter', '<', 'year', '>', 'presentation', 'included', 'a', 'chart', 'showing', 'l', '##ng', 'over', '##cap', '##ac', '##ity', 'for', 'some', 'years', 'which', 'in', 'turn', 'should', 'be', 'encouraging', 'for', 'l', '##ng', 'import', '##ers', 'and', 'f', '##sr', '##u', 'operators', 'updated', 'so', '##tp', 'and', 'attractive', 'divide', '##nds', 'coming', 'as', '<', 'company', '>', 'energy', 'is', 'an', 'adequately', 'traded', 'stock', 'there', 'is', 'no', 'reason', 'to', 'penal', '##ise', '<', 'company', '>', 'for', 'its', 'holding', 'we', 'estimate', '<', 'company', '>', 'will', 'generate', '<', 'currency', '>', '<', 'number', '_', 'range', '>', 'million', 'of', 'cash', 'flow', 'just', 'above', '<', 'currency', '>', '<', 'number', '>', 'per', 'share', 'annually', 'of', 'free', 'cash', 'after', 'debt', 'amor', '##tis', '##ation', 'our', 'updated', 'so', '##tp', 'returns', '<', 'currency', '>', '<', 'number', '>', 'per', 'share', 'down', 'from', '<', 'number', '>', 'the', 'changes', 'relate', 'to', 'the', 'price', 'of', 'go', '##le', 'we', 'rei', '##tera', '##te', 'our', 'buy', 'rating', 'with', 'a', 'revised', 'target', 'price', 'of', '<', 'currency', '>', '<', 'number', '>']\n",
      "Token IDs:  [2959, 4284, 2220, 3467, 13695, 16565, 2077, 3037, 1998, 7773, 1026, 2194, 1028, 2988, 1037, 2959, 4284, 1026, 2095, 1028, 16565, 2077, 3037, 7773, 2139, 28139, 23247, 1998, 16095, 3775, 9276, 1997, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 2114, 2256, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 19939, 1998, 1996, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 10465, 9934, 1996, 3653, 4171, 24353, 2015, 2020, 2714, 1996, 2488, 2084, 3517, 3463, 2020, 3303, 2011, 3020, 16565, 2005, 1996, 3962, 6202, 1997, 6470, 1999, 2049, 2684, 2194, 1026, 2194, 1028, 2943, 2968, 7127, 2008, 21183, 24411, 3370, 1999, 1996, 3962, 3006, 2097, 2022, 2659, 2005, 1996, 2034, 2431, 1997, 1026, 2095, 1028, 1998, 2004, 1037, 9509, 2057, 2031, 3013, 2256, 1026, 2095, 1028, 16565, 2566, 3745, 19939, 2011, 1026, 3867, 1028, 2174, 11443, 4859, 10504, 2097, 2707, 3041, 1999, 2117, 4284, 1026, 2095, 1028, 2612, 1997, 2353, 4284, 1026, 2095, 1028, 10831, 1998, 7826, 1026, 2194, 1028, 23010, 4321, 2202, 2729, 1997, 2049, 5356, 4834, 2029, 2057, 2903, 2024, 8702, 2135, 21125, 1999, 1996, 3006, 2057, 2036, 5136, 1996, 3431, 1999, 1996, 2684, 2194, 1026, 2194, 1028, 2943, 2029, 2057, 2079, 2025, 3104, 2000, 2022, 2204, 2005, 2175, 8017, 15337, 1996, 2194, 2038, 6153, 2185, 2013, 3375, 8274, 1048, 3070, 2537, 1998, 4866, 10518, 1999, 5622, 4226, 7011, 7542, 2612, 1996, 2194, 2038, 6025, 16565, 4208, 2006, 1042, 21338, 2271, 2029, 2031, 6022, 7820, 2599, 2335, 2062, 3563, 8616, 2015, 1998, 2896, 4087, 3891, 1996, 2959, 4284, 1026, 2095, 1028, 8312, 2443, 1037, 3673, 4760, 1048, 3070, 2058, 17695, 6305, 3012, 2005, 2070, 2086, 2029, 1999, 2735, 2323, 2022, 11434, 2005, 1048, 3070, 12324, 2545, 1998, 1042, 21338, 2226, 9224, 7172, 2061, 25856, 1998, 8702, 11443, 18376, 2746, 2004, 1026, 2194, 1028, 2943, 2003, 2019, 23613, 7007, 4518, 2045, 2003, 2053, 3114, 2000, 18476, 5562, 1026, 2194, 1028, 2005, 2049, 3173, 2057, 10197, 1026, 2194, 1028, 2097, 9699, 1026, 9598, 1028, 1026, 2193, 1035, 2846, 1028, 2454, 1997, 5356, 4834, 2074, 2682, 1026, 9598, 1028, 1026, 2193, 1028, 2566, 3745, 6604, 1997, 2489, 5356, 2044, 7016, 16095, 7315, 3370, 2256, 7172, 2061, 25856, 5651, 1026, 9598, 1028, 1026, 2193, 1028, 2566, 3745, 2091, 2013, 1026, 2193, 1028, 1996, 3431, 14396, 2000, 1996, 3976, 1997, 2175, 2571, 2057, 24964, 14621, 2618, 2256, 4965, 5790, 2007, 1037, 8001, 4539, 3976, 1997, 1026, 9598, 1028, 1026, 2193, 1028]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print('Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2bBdb3pt8LuQ",
    "outputId": "043462be-49c9-4a91-92c7-268acf61a6d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa65c7ee8de4166971ae716a329f404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:  fourth quarter  early winter lifts earnings before interest and taxes  <company> reported a fourth quarter <year> earnings before interest  taxes  depreciation  and amortization of <currency> <number> million  against our <currency> <number> million forecast and the <currency> <number> million consensus  inquiry   the pre tax deviations were similar  the better than expected results were caused by higher earnings for the spot trading of vessels in its daughter company  <company> energy  management indicates that utilisation in the spot market will be low for the first half of <year> and as a consequence we have cut our <year> earnings per share forecast by <percent>  however  dividend payments will start earlier  in second quarter <year> instead of third quarter <year>  risks and directions  <company> charters largely take care of its cash flow  which we believe are attractively priced in the market  we also consider the changes in the daughter company <company> energy  which we do not cover  to be good for golar shareholders  the company has backed away from complex floating lng production and extensive investments in liquefaction  instead  the company has retained earnings focused on fsrus  which have significantly shorter lead times  more specific tenders and lower technical risk  the fourth quarter <year> presentation included a chart showing lng overcapacity for some years  which in turn should be encouraging for lng importers and fsru operators  updated sotp and attractive dividends coming  as <company> energy is an adequately traded stock  there is no reason to penalise <company> for its holding  we estimate <company> will generate <currency> <number_range>  million of cash flow  just above <currency> <number> per share annually of free cash  after debt amortisation  our updated sotp returns <currency> <number> per share  down from <number>  the changes relate to the price of gole  we reiterate our buy rating  with a revised target price of <currency> <number>  \n",
      "Token IDs: [101, 2959, 4284, 2220, 3467, 13695, 16565, 2077, 3037, 1998, 7773, 1026, 2194, 1028, 2988, 1037, 2959, 4284, 1026, 2095, 1028, 16565, 2077, 3037, 7773, 2139, 28139, 23247, 1998, 16095, 3775, 9276, 1997, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 2114, 2256, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 19939, 1998, 1996, 1026, 9598, 1028, 1026, 2193, 1028, 2454, 10465, 9934, 1996, 3653, 4171, 24353, 2015, 2020, 2714, 1996, 2488, 2084, 3517, 3463, 2020, 3303, 2011, 3020, 16565, 2005, 1996, 3962, 6202, 1997, 6470, 1999, 2049, 2684, 2194, 1026, 2194, 1028, 2943, 2968, 7127, 2008, 21183, 24411, 3370, 1999, 1996, 3962, 3006, 2097, 2022, 2659, 2005, 1996, 2034, 2431, 1997, 1026, 2095, 1028, 1998, 2004, 1037, 9509, 2057, 2031, 3013, 2256, 1026, 2095, 1028, 16565, 2566, 3745, 19939, 2011, 1026, 3867, 1028, 2174, 11443, 4859, 10504, 2097, 2707, 3041, 1999, 2117, 4284, 1026, 2095, 1028, 2612, 1997, 2353, 4284, 1026, 2095, 1028, 10831, 1998, 7826, 1026, 2194, 1028, 23010, 4321, 2202, 2729, 1997, 2049, 5356, 4834, 2029, 2057, 2903, 2024, 8702, 2135, 21125, 1999, 1996, 3006, 2057, 2036, 5136, 1996, 3431, 1999, 1996, 2684, 2194, 1026, 2194, 1028, 2943, 2029, 2057, 2079, 2025, 3104, 2000, 2022, 2204, 2005, 2175, 8017, 15337, 1996, 2194, 2038, 6153, 2185, 2013, 3375, 8274, 1048, 3070, 2537, 1998, 4866, 10518, 1999, 5622, 4226, 7011, 7542, 2612, 1996, 2194, 2038, 6025, 16565, 4208, 2006, 1042, 21338, 2271, 2029, 2031, 6022, 7820, 2599, 2335, 2062, 3563, 8616, 2015, 1998, 2896, 4087, 3891, 1996, 2959, 4284, 1026, 2095, 1028, 8312, 2443, 1037, 3673, 4760, 1048, 3070, 2058, 17695, 6305, 3012, 2005, 2070, 2086, 2029, 1999, 2735, 2323, 2022, 11434, 2005, 1048, 3070, 12324, 2545, 1998, 1042, 21338, 2226, 9224, 7172, 2061, 25856, 1998, 8702, 11443, 18376, 2746, 2004, 1026, 2194, 1028, 2943, 2003, 2019, 23613, 7007, 4518, 2045, 2003, 2053, 3114, 2000, 18476, 5562, 1026, 2194, 1028, 2005, 2049, 3173, 2057, 10197, 1026, 2194, 1028, 2097, 9699, 1026, 9598, 1028, 1026, 2193, 1035, 2846, 1028, 2454, 1997, 5356, 4834, 2074, 2682, 1026, 9598, 1028, 1026, 2193, 1028, 2566, 3745, 6604, 1997, 2489, 5356, 2044, 7016, 16095, 7315, 3370, 2256, 7172, 2061, 25856, 5651, 1026, 9598, 1028, 1026, 2193, 1028, 2566, 3745, 2091, 2013, 1026, 2193, 1028, 1996, 3431, 14396, 2000, 1996, 3976, 1997, 2175, 2571, 2057, 24964, 14621, 2618, 2256, 4965, 5790, 2007, 1037, 8001, 4539, 3976, 1997, 1026, 9598, 1028, 1026, 2193, 1028, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Att Masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_LENGTH = 500\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    encoded_sent = tokenizer.encode_plus(\n",
    "                sent,            \n",
    "                add_special_tokens=True, \n",
    "                max_length=MAX_LENGTH, \n",
    "                pad_to_max_length=True\n",
    "    )\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent['input_ids'])\n",
    "    attention_masks.append(encoded_sent['attention_mask'])\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Att Masks:', attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhUZO9vc_l6T",
    "outputId": "20d43d20-5b2c-4c51-f3b1-dd59f20646da"
   },
   "outputs": [],
   "source": [
    " print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "# We will call the train_test_split() function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=46, test_size=0.2)\n",
    "# Performing same steps on the attention masks\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=46, test_size=0.2)\n",
    "\n",
    "train_labels_temp = copy.deepcopy(train_labels)\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_inputs, train_labels, \n",
    "                                                            random_state=46, test_size=0.125)\n",
    "# Performing same steps on the attention masks\n",
    "train_masks, validation_masks, _, _ = train_test_split(train_masks, train_labels_temp,\n",
    "                                             random_state=46, test_size=0.125)\n",
    "\n",
    "del train_labels_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jw5K2A5Ko1RF"
   },
   "outputs": [],
   "source": [
    "#Converting the input data to the tensor , which can be feeded to the model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balancing vs No class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GEgLpFVlo1Z-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17667, 500])\n",
      "torch.Size([17667, 500])\n",
      "torch.Size([17667])\n"
     ]
    }
   ],
   "source": [
    "#Creating the DataLoader which will help us to load data into the GPU/CPU\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "print(train_inputs.size())\n",
    "print(train_masks.size())\n",
    "print(train_labels.size())\n",
    "weights = 1. / torch.tensor([84,53,14], dtype=torch.float)\n",
    "\n",
    "samples_weights = weights[train_data.tensors[2]]\n",
    "\n",
    "# print(samples_weights.size())\n",
    "# print(samples_weights)\n",
    "# temp_numpy = samples_weights.numpy()\n",
    "# print(np.unique(temp_numpy, return_counts=True))\n",
    "\n",
    "# train_sampler = WeightedRandomSampler(\n",
    "#     weights=samples_weights,\n",
    "#     num_samples=int(len(train_data) * 1.5),\n",
    "#     replacement=True)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for validation and test set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gFsCTp_mporB",
    "outputId": "60b3ee08-16e0-464a-f784-684df32628c3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the pre-trained BERT model from huggingface library\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, AlbertForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "# model = RobertaForSequenceClassification.from_pretrained(\n",
    "#     \"/home/jupyter/pretrained-models/roberta-large/\", \n",
    "#     num_labels = 3,   \n",
    "#     output_attentions = False, \n",
    "#     output_hidden_states = False, )\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"/home/jupyter/pretrained-models/fin-bert/\", \n",
    "    num_labels = 3,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, )\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/jupyter/saved-models/roberta-large-500-finetuned.pth\"))\n",
    "# uncomment this if need to load our models\n",
    "# Teeling the model to run on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the bottom layers\n",
    "### Need to change unfreeze_layers with different backbones(names might be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([3, 768])\n",
      "classifier.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# unfreeze the top layers and heads\n",
    "# unfreeze_layers = ['layer.8','layer.9','layer.10','layer.11','roberta.pooler','classifier.'] # roberta-base\n",
    "# unfreeze_layers = ['layer.16','layer.17','layer.18','layer.19','layer.20','layer.21','layer.22','layer.23','roberta.pooler','classifier.'] # roberta-large\n",
    "unfreeze_layers = ['layer.8','layer.9','layer.10','layer.11','bert.pooler','classifier.'] # fin-bert\n",
    "\n",
    "for name ,param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for ele in unfreeze_layers:\n",
    "        if ele in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "            \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-p0upAhhRiIx",
    "outputId": "3a075bc4-4570-4f37-b8f8-1d31a91d32a9"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 15\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "#Creating the helper function to have a watch on elapsed time\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6J-FYdx6nFE_",
    "outputId": "40adc853-1e64-49ef-a340-7c033918903a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f931140e7c3041f1a401c6db0820416b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.71833\n",
      "  Training epoch took: 0:08:40\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd464376826416cb77e3d888ca0caf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.81250\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb038c8654b454ab721f6522ee4d9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.40358\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42cf509869a40cb9955e7182a04a46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.82265\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3182bef6b29d465087f5572134e8c442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.37211\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25324599941d4d9b8d2c747b5eaae802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.82476\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539854febc004f6b8f037be564c44a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.34273\n",
      "  Training epoch took: 0:08:38\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66939e884a0d4f4e8838ded9400e88ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.82911\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784c57cb92e3438da537f926c3213cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.31883\n",
      "  Training epoch took: 0:08:38\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978037a1ff44083a547431891ce1842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.83452\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93fb89860c94b328d076deaf45fd3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.29550\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3904214ac3a49b2ae49cba58247f120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.83188\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b80b04a4bd9433c9582274c937837b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.27257\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b327a72b145c49a69c1ea5fe7e74128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.83716\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c6541815994708a514f463a3c7ca64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.25210\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95249fb92f644dafbf84b4963af9bf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84045\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b196f421da32499387c5f6b4a75cd54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.23481\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720370ef78af4df9ac65fe7bb1f4cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.83716\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8870b49f24224555af5f33cb2d959923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.21562\n",
      "  Training epoch took: 0:08:38\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf7a33b20174fb7bbb97ed27a2ae445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.82397\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da03ad90f6d4c5f907ac00092378925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.20018\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afa620f186641de82ca1dd0a63281ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84520\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ab93de5793450dbf5373ff6ca3d089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.18299\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f78fc1ca5e547c5a3272fad0aa06397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84639\n",
      "  Validation took: 0:00:43\n",
      "  Model saved.\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b690213c2484d4e80fafc25f941cc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.17489\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d17a66f47f4394876fce9d65246715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84309\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f19f73822b485abf05f5386ae03506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.16706\n",
      "  Training epoch took: 0:08:38\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f57be8fde0409493181a7b5c74b99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84151\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e9401771d240008432200a2952a2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.16022\n",
      "  Training epoch took: 0:08:39\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dee37ab5b5444c4a4d1172ae578195b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84309\n",
      "  Validation took: 0:00:43\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in tqdm(enumerate(train_dataloader),bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\"):\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    preds = None\n",
    "    labels = None\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        targets_np = b_labels.detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = logits\n",
    "        else:\n",
    "            preds = np.append(preds,logits, axis=0)\n",
    "\n",
    "        if labels is None:\n",
    "            labels = targets_np\n",
    "        else:\n",
    "            labels = np.append(labels, targets_np)\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    if eval_accuracy/nb_eval_steps > best_acc:\n",
    "        torch.save(model.state_dict(), \"/home/jupyter/saved-models/fin-bert-500-finetuned.pth\")\n",
    "        best_acc = eval_accuracy/nb_eval_steps\n",
    "        print(\"  Model saved.\")\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    predictions_list.append(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1362.,    0.,    0.,    0.,    0.,  987.,    0.,    0.,    0.,\n",
       "         175.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAASU0lEQVR4nO3cbYxc53ne8f9V0lb8EsVSuVJZkg2ZgkhDGgksL1jFLgIXaiHGSkwVqIA1mphIBRAx1NYp2qZkAlifCCho0aYGKheErZpCXQmEY0dEHLkWmARGa1vKSpYtUTSjjaWKGzLiJm5jOSmUUr37YY6Q6WqWOy87s2af/w9YzJn7PM+cewcPrz0885KqQpLUhr+02Q1IkmbH0Jekhhj6ktQQQ1+SGmLoS1JDtm52A+vZtm1b7d69e7PbkKRrypNPPvlHVTW3uv49H/q7d+9mcXFxs9uQpGtKkv8+qL7u5Z0kDyS5nOTZAfv+eZJKsq2vdizJUpLzSW7vq787yTPdvo8lybi/jCRpPMNc0/8UcHB1Mcku4O8CL/XV9gELwP5uzv1JtnS7Pw4cAfZ2P294TEnSdK0b+lX1JeDbA3b9W+AXgf6P9B4CHq6qV6vqBWAJOJBkO3B9VX2leh8BfhC4c9LmJUmjGevdO0k+APxBVX191a4dwIW++8tdbUe3vbq+1uMfSbKYZHFlZWWcFiVJA4wc+kneCvwy8NFBuwfU6ir1garqRFXNV9X83NwbXnyWJI1pnHfv/HVgD/D17rXYncBTSQ7QO4Pf1Td2J3Cxq+8cUJckzdDIZ/pV9UxV3VRVu6tqN71Av6Wq/hA4DSwkuS7JHnov2D5RVZeAV5Lc2r1r50PAIxv3a0iShjHMWzYfAr4C/HCS5SR3rzW2qs4Cp4DngC8A91TVa93uDwOfoPfi7u8Dj07YuyRpRPle/z79+fn58sNZkjSaJE9W1fzq+vf8J3Insfvo5zfluC/ed8emHFeS1uMXrklSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSHrhn6SB5JcTvJsX+1fJflmkm8k+VySd/TtO5ZkKcn5JLf31d+d5Jlu38eSZMN/G0nSVQ1zpv8p4OCq2mPAO6vqR4HfA44BJNkHLAD7uzn3J9nSzfk4cATY2/2sfkxJ0pStG/pV9SXg26tqX6yqK93drwI7u+1DwMNV9WpVvQAsAQeSbAeur6qvVFUBDwJ3btDvIEka0kZc0/+HwKPd9g7gQt++5a62o9teXR8oyZEki0kWV1ZWNqBFSRJMGPpJfhm4Anz69dKAYXWV+kBVdaKq5qtqfm5ubpIWJUl9to47Mclh4KeA27pLNtA7g9/VN2wncLGr7xxQlyTN0Fhn+kkOAv8S+EBV/VnfrtPAQpLrkuyh94LtE1V1CXglya3du3Y+BDwyYe+SpBGte6af5CHgfcC2JMvAvfTerXMd8Fj3zsuvVtXPV9XZJKeA5+hd9rmnql7rHurD9N4J9BZ6rwE8iiRpptYN/ar64IDyJ68y/jhwfEB9EXjnSN1JkjaUn8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNWTf0kzyQ5HKSZ/tqNyZ5LMnz3e0NffuOJVlKcj7J7X31dyd5ptv3sSTZ+F9HknQ1w5zpfwo4uKp2FDhTVXuBM919kuwDFoD93Zz7k2zp5nwcOALs7X5WP6YkacrWDf2q+hLw7VXlQ8DJbvskcGdf/eGqerWqXgCWgANJtgPXV9VXqqqAB/vmSJJmZNxr+jdX1SWA7vamrr4DuNA3brmr7ei2V9cHSnIkyWKSxZWVlTFblCStttEv5A66Tl9XqQ9UVSeqar6q5ufm5jasOUlq3dYx572cZHtVXeou3Vzu6svArr5xO4GLXX3ngLp0zdp99PObduwX77tj046ta9u4Z/qngcPd9mHgkb76QpLrkuyh94LtE90loFeS3Nq9a+dDfXMkSTOy7pl+koeA9wHbkiwD9wL3AaeS3A28BNwFUFVnk5wCngOuAPdU1WvdQ32Y3juB3gI82v1IkmZo3dCvqg+useu2NcYfB44PqC8C7xypO0nShvITuZLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNmSj0k/zTJGeTPJvkoSTfl+TGJI8leb67vaFv/LEkS0nOJ7l98vYlSaMYO/ST7AD+CTBfVe8EtgALwFHgTFXtBc5090myr9u/HzgI3J9ky2TtS5JGMenlna3AW5JsBd4KXAQOASe7/SeBO7vtQ8DDVfVqVb0ALAEHJjy+JGkEY4d+Vf0B8K+Bl4BLwJ9U1ReBm6vqUjfmEnBTN2UHcKHvIZa72hskOZJkMcniysrKuC1KklaZ5PLODfTO3vcAfxV4W5KfudqUAbUaNLCqTlTVfFXNz83NjduiJGmVSS7v/B3ghapaqar/DXwWeA/wcpLtAN3t5W78MrCrb/5OepeDJEkzMknovwTcmuStSQLcBpwDTgOHuzGHgUe67dPAQpLrkuwB9gJPTHB8SdKIto47saoeT/IZ4CngCvA14ATwduBUkrvp/WG4qxt/Nskp4Llu/D1V9dqE/UuSRjB26ANU1b3AvavKr9I76x80/jhwfJJjSpLG5ydyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhoyUegneUeSzyT5ZpJzSX48yY1JHkvyfHd7Q9/4Y0mWkpxPcvvk7UuSRjHpmf6/A75QVX8D+DHgHHAUOFNVe4Ez3X2S7AMWgP3AQeD+JFsmPL4kaQRjh36S64GfAD4JUFV/XlX/EzgEnOyGnQTu7LYPAQ9X1atV9QKwBBwY9/iSpNFNcqb/Q8AK8B+TfC3JJ5K8Dbi5qi4BdLc3deN3ABf65i93NUnSjEwS+luBW4CPV9W7gD+lu5Szhgyo1cCByZEki0kWV1ZWJmhRktRvktBfBpar6vHu/mfo/RF4Ocl2gO72ct/4XX3zdwIXBz1wVZ2oqvmqmp+bm5ugRUlSv7FDv6r+ELiQ5Ie70m3Ac8Bp4HBXOww80m2fBhaSXJdkD7AXeGLc40uSRrd1wvn/GPh0kjcD3wJ+jt4fklNJ7gZeAu4CqKqzSU7R+8NwBbinql6b8PiSpBFMFPpV9TQwP2DXbWuMPw4cn+SYkqTx+YlcSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkIlDP8mWJF9L8hvd/RuTPJbk+e72hr6xx5IsJTmf5PZJjy1JGs1GnOl/BDjXd/8ocKaq9gJnuvsk2QcsAPuBg8D9SbZswPElSUOaKPST7ATuAD7RVz4EnOy2TwJ39tUfrqpXq+oFYAk4MMnxJUmjmfRM/1eBXwT+T1/t5qq6BNDd3tTVdwAX+sYtd7U3SHIkyWKSxZWVlQlblCS9buzQT/JTwOWqenLYKQNqNWhgVZ2oqvmqmp+bmxu3RUnSKlsnmPte4ANJ3g98H3B9kv8EvJxke1VdSrIduNyNXwZ29c3fCVyc4PiSpBGNfaZfVceqamdV7ab3Au1vVdXPAKeBw92ww8Aj3fZpYCHJdUn2AHuBJ8buXJI0sknO9NdyH3Aqyd3AS8BdAFV1Nskp4DngCnBPVb02heNLktawIaFfVb8D/E63/cfAbWuMOw4c34hjSpJG5ydyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhoydugn2ZXkt5OcS3I2yUe6+o1JHkvyfHd7Q9+cY0mWkpxPcvtG/AKSpOFNcqZ/BfhnVfUjwK3APUn2AUeBM1W1FzjT3afbtwDsBw4C9yfZMknzkqTRjB36VXWpqp7qtl8BzgE7gEPAyW7YSeDObvsQ8HBVvVpVLwBLwIFxjy9JGt2GXNNPsht4F/A4cHNVXYLeHwbgpm7YDuBC37Tlrjbo8Y4kWUyyuLKyshEtSpLYgNBP8nbg14BfqKrvXG3ogFoNGlhVJ6pqvqrm5+bmJm1RktSZKPSTvIle4H+6qj7blV9Osr3bvx243NWXgV1903cCFyc5viRpNJO8eyfAJ4FzVfVv+nadBg5324eBR/rqC0muS7IH2As8Me7xJUmj2zrB3PcCPws8k+TprvZLwH3AqSR3Ay8BdwFU1dkkp4Dn6L3z556qem2C40vS1O0++vlNOe6L990xlccdO/Sr6r8y+Do9wG1rzDkOHB/3mJKkyfiJXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JCZh36Sg0nOJ1lKcnTWx5ekls009JNsAf498JPAPuCDSfbNsgdJatmsz/QPAEtV9a2q+nPgYeDQjHuQpGZtnfHxdgAX+u4vA39z9aAkR4Aj3d3vJjk/5vG2AX805tyx5VfWHbIpfQ3BvkazaX2ts8Z8vkbzPdlXfmXivn5wUHHWoZ8BtXpDoeoEcGLigyWLVTU/6eNsNPsajX2Nxr5G01pfs768swzs6ru/E7g44x4kqVmzDv3fBfYm2ZPkzcACcHrGPUhSs2Z6eaeqriT5R8B/AbYAD1TV2SkecuJLRFNiX6Oxr9HY12ia6itVb7ikLkn6/5SfyJWkhhj6ktSQazL01/sqh/R8rNv/jSS3DDt3yn39g66fbyT5cpIf69v3YpJnkjydZHHGfb0vyZ90x346yUeHnTvlvv5FX0/PJnktyY3dvmk+Xw8kuZzk2TX2b9b6Wq+vzVpf6/W1Wetrvb42a33tSvLbSc4lOZvkIwPGTG+NVdU19UPvBeDfB34IeDPwdWDfqjHvBx6l97mAW4HHh5075b7eA9zQbf/k6311918Etm3S8/U+4DfGmTvNvlaN/2ngt6b9fHWP/RPALcCza+yf+foasq+Zr68h+5r5+hqmr01cX9uBW7rt7wd+b5YZdi2e6Q/zVQ6HgAer56vAO5JsH3Lu1Pqqqi9X1f/o7n6V3ucUpm2S33lTn69VPgg8tEHHvqqq+hLw7asM2Yz1tW5fm7S+hnm+1rKpz9cqs1xfl6rqqW77FeAcvW8r6De1NXYthv6gr3JY/YStNWaYudPsq9/d9P6Sv66ALyZ5Mr2vodgow/b140m+nuTRJPtHnDvNvkjyVuAg8Gt95Wk9X8PYjPU1qlmtr2HNen0NbTPXV5LdwLuAx1ftmtoam/XXMGyEYb7KYa0xQ30NxJiGfuwkf5veP8q/1Vd+b1VdTHIT8FiSb3ZnKrPo6yngB6vqu0neD/w6sHfIudPs63U/Dfy3quo/a5vW8zWMzVhfQ5vx+hrGZqyvUWzK+krydnp/aH6hqr6zeveAKRuyxq7FM/1hvsphrTHT/BqIoR47yY8CnwAOVdUfv16vqovd7WXgc/T+GzeTvqrqO1X13W77N4E3Jdk2zNxp9tVngVX/9Z7i8zWMzVhfQ9mE9bWuTVpfo5j5+kryJnqB/+mq+uyAIdNbY9N4oWKaP/T+d/ItYA9/8ULG/lVj7uD/fRHkiWHnTrmvvwYsAe9ZVX8b8P19218GDs6wr7/CX3xQ7wDwUvfcberz1Y37AXrXZd82i+er7xi7WfuFyZmvryH7mvn6GrKvma+vYfrarPXV/e4PAr96lTFTW2PX3OWdWuOrHJL8fLf/PwC/Se/V7yXgz4Cfu9rcGfb1UeAvA/cnAbhSvW/Ruxn4XFfbCvznqvrCDPv6+8CHk1wB/hewUL0VttnPF8DfA75YVX/aN31qzxdAkofoveNkW5Jl4F7gTX19zXx9DdnXzNfXkH3NfH0N2RdswvoC3gv8LPBMkqe72i/R+6M99TXm1zBIUkOuxWv6kqQxGfqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIf8X/CvmZ0ilqMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(predictions_list[-1])\n",
    "plt.hist(predictions_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set (Real performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3fdfebe2104f00835282a5ba75105c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=316.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Accuracy: 0.84335\n",
      "  Testing took: 0:01:25\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"/home/jupyter/saved-models/fin-bert-500-finetuned.pth\"))\n",
    "\n",
    "model.eval()\n",
    "t0 = time.time()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "preds = None\n",
    "labels = None\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    targets_np = b_labels.detach().cpu().numpy()\n",
    "\n",
    "    if preds is None:\n",
    "        preds = logits\n",
    "    else:\n",
    "        preds = np.append(preds,logits, axis=0)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = targets_np\n",
    "    else:\n",
    "        labels = np.append(labels, targets_np)\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Accumulate the total accuracy.\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    # Track the number of batches\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"  Testing took: {:}\".format(format_time(time.time() - t0)))\n",
    "# if eval_accuracy/nb_eval_steps > best_acc:\n",
    "#     torch.save(model.state_dict(), \"/home/jupyter/saved-models/roberta-base-500-finetuned.pth\")\n",
    "#     best_acc = eval_accuracy/nb_eval_steps\n",
    "#     print(\"  Model saved.\")\n",
    "# preds = np.argmax(preds, axis=1)\n",
    "# predictions_list.append(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best results on DEV set\n",
    "\n",
    "#### Results with origional preproseccing:\n",
    "\n",
    "roberta-base :76.7(4layer)\n",
    "\n",
    "roberta-large: 77.5(5layer)\n",
    "\n",
    "roberta-large: 78.9(8layer)\n",
    "\n",
    "fin-bert: 76.4(4layer)\n",
    "\n",
    "#### Results with random replaced tokens:\n",
    "\n",
    "roberta-base :76.6(4layer)\n",
    "\n",
    "roberta-large: 79.1(8layer)\n",
    "\n",
    "fin-bert: 76.3(6layer)\n",
    "\n",
    "fin-bert: 75.3(4layer)\n",
    "\n",
    "#### Results with origional preproseccing (MAX LENGTH = 350):\n",
    "\n",
    "roberta-base:81.6(4layer)\n",
    "\n",
    "roberta-large: 82.7(8layer)\n",
    "\n",
    "fin-bert:81.6(4layer)\n",
    "\n",
    "#### Results with origional preproseccing (MAX LENGTH = 500):\n",
    "\n",
    "roberta-base: 85.7(4layer)\n",
    "\n",
    "roberta-large: 85.9(8layer)\n",
    "\n",
    "### Best results on TEST set\n",
    "fin-bert:84.3(4layer)\n",
    "\n",
    "roberta-base: 84.6(4layer)\n",
    "\n",
    "roberta-large: 86.1(8layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balancing didnt make the result better."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
